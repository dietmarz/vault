include::{root}/.inc/include.adoc[]

= Apache Kafka Grundlagen


== Begriffe
* Kafka Cluster besteht aus Brokern die Replikate beinhalten
* Kafka has the concept of topic, topics bestehen aus partitions (resiliency)

* `Broker` (Server) vermitteln zwischen Producern und Consumern anhand von Topics
* `Topic` (Kontext) hat Name, gliedern sich in Partitionen, wie ein Stream von Daten.
  ** no queries, immutable, contains N Partitions,
  ** Offset (~Message ID) hat nur eine Bedeutung innerhalb einer Partition
* `Partion` verwaltet von Kafka, single-threaded bearbeitet, immutalbe
  ** Msgs are ordered only within one Partition
  ** Hat max ein Consumer pro Consumer-Group

* `Message` = Key? + Value? + CompressionType + Headers? + (Partition + Offset) + Timestamp // ? = nullable
* `Producer` Weiß in welche Partition geschrieben wird
  ** msg.key == null ? Partition RoundRobin : same Partition (ordered!) // truck-id -> key

* `Consumer`
  ** Wenn ein Consumer N Partitions liest, dann eine nach der anderen.
  ** Nie 2 Consumer einer Group für dieselbe Partition, #Consumer <= #Partitions pro Consumer-Group
  ** Weiß vorher in welchem Format die Binärdaten kommen
  ** Wenn alle Partitions einen Consumer haben, bleibt der neue Consumer inactive

* `Consumer-Groups` lesen alle Messages eines Topics genau einmal
  ** Kafka assigns partitions of that topic to the consumers in the group.
  ** Kafka rebalances the partition assignment among the consumers automatically
  ** Delivery semantics describe how Kafka guarantees message delivery to consumers.
     *** `at most once` Once or not at all: Risk of failure occurs after a message is sent but before it is acknowledged.
     *** `at least once` One or more times: duplicates are possible if a failure occurs and the message is resent
     *** `exactly once` Additional configuration needed. For critical applications where neither message loss nor duplication is acceptable.

* `Zookeeper` überwacht Kafka, Kafka beinhaltet N Broker
  ** Kafka without `Zookeeper` (KIP-500) using `KRaft` instead. Zwang bis 2.X, optional ab 3.X, Not at 4.X <<ak>>(14)
  ** ZK uses a leader, rest are follower. Don`t use ZK to configure your K Clients.

* Kafka stores per partition in a Topic named  `__consumer_offsets` at which a consumer group has been reading
  ** Key ist combination: GroupName-TopicName-PartitionNumber
  ** enable.auto.commit == true ? Every 5sec : Every commitSync() or commitAsync() call

* `bootstrap server` = every broker, sends on Metadata-Request a list of all brokers
* Replication factor N means (N-1) broker can loss.
* Only one broker is the `leader` which is allowed to receive data from a producer. All other are replica <<ak>>(12)
* `Producer Acks` acks=0 => Producer won't waint, acks=1 => Producer wait for the Leader, acks=all => Producer wait for all replicas <<ak>>(13)

* 'KRaft` due to scaling issues (>100.000 Part) Prod ready since K 3.3.1 (KIP-833)  <<ak>>(15)
  ** One Broker gets the `Quorum Controller` instead of having a Zookeeper

* Zookeeper speichert: Broker ID, Hostname, Port, dazu T_Name jeweilige Partitions.
  ** Liste activer Broker, Leader Election, Verteilt Configs, Consumer Group Management

.<<ak>>(16)
----
Source -> Producer → Kafka Cluster --------------> Consumers -> Target Systems
                     ↑   Broker: B_ID
                     |       Topic: T_Name
                     |           Partition: T_Name-P_Nr
                     Zookeeper
----


Weiter: Im Kurs statt installation Docker benutzen
  ~/project/kafka/ud_apache-kafka/kafka-stack-docker-compose
Statt dem kafka für den Client local zu installieren auch docker benutzen


.Aus Ud Clean architecture
[source,bash]
----
cd ~/project/architecture/ud-clean-arch-saga/food-ordering-system/infrastructure/docker-compose

# Start and check Zookeeper
docker-compose -f common.yml -f zookeeper.yml up
echo ruok | netcat localhost 2181  # Check healthy ruok (AreYourOK) returns imok (I am OK) nc = netcat

# Start Cluster
docker-compose -f common.yml -f kafka_cluster.yml up

# Start init Kafka
docker-compose -f common.yml -f init_kafka.yml up

# Add cluster manually
chromium http://localhost:9000/addCluster
# Cluster Name: food-ordering-system-cluster
# Cluster Zookeepers Hosts: zookeeper:2181 -> [Save-Button]
# Check in Cluster view:  Topics 7, Broker 3
----


[bibliography]
- [[[ak]]] https://www.udemy.com/course/apache-kafka[Apache-Kafka]
- [[[try1]]] https://github.com/streamthoughts/kafka-monitoring-stack-docker-compose[Example Docker 1]
- [[[try2]]] https://github.com/conduktor/kafka-stack-docker-compose[Example Docker 2]
- [[[arch]]] Udemy Course https://www.udemy.com/course/microservices-clean-architecture-ddd-saga-outbox-kafka-kubernetes/[Arch]


